#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSeek-V2 MLA Demo â€” è¡Œå†…å…¬å¼æ³¨é‡Šç‰ˆï¼ˆå®Œæ•´ç‰ˆï¼‰
éªŒè¯ MLA çš„ä½ç§©é”®å€¼å‹ç¼© + è§£è€¦ RoPE
ä½œè€…ï¼šChatGPT (GPT-5)
"""

import torch
import torch.nn.functional as F


# =========================================================
# ğŸ”¹ RoPE æ—‹è½¬å‡½æ•°
# å…¬å¼: RoPE(x) = xÂ·cosÎ¸ + rotate(x)Â·sinÎ¸
# =========================================================
def apply_rope_x(x, cos, sin):
    x1, x2 = x[..., ::2], x[..., 1::2]                            # æ‹†åˆ†å¶/å¥‡ç»´åº¦ â†’ x=[xâ‚, xâ‚‚]
    x_rot = torch.stack([-x2, x1], dim=-1).reshape_as(x)          # rotate(x) = [-xâ‚‚, xâ‚]
    return x * cos + x_rot * sin                                  # å¼(R): RoPE(x)=xÂ·cosÎ¸+rotate(x)Â·sinÎ¸


# =========================================================
# ğŸ”¹ MLA æ¨¡å—
# =========================================================
class MLA(torch.nn.Module):
    def __init__(self, d_model, n_heads, max_len=1024, rope_theta=10000.0):
        super().__init__()
        self.d_model = d_model                                     # æ¨¡å‹ç»´åº¦ d_model
        self.n_heads = n_heads                                     # æ³¨æ„åŠ›å¤´æ•° n_h
        self.dh = d_model // n_heads                               # æ¯ä¸ª head çš„ç»´åº¦ d_h

        self.q_proj_dim = d_model // 2                             # å¼(QD): c_t^{Q} = W^{DQ} h_t
        self.kv_proj_dim = (2 * d_model) // 3                      # å¼(9):  c_t^{KV} = W^{DKV} h_t

        self.qk_nope_dim = self.dh // 2                            # ä¸å¸¦RoPEéƒ¨åˆ† â†’ q_t^{C}, k_j^{C}
        self.qk_rope_dim = self.dh // 2                            # å¸¦RoPEéƒ¨åˆ† â†’ q_t^{R}, k_j^{R}

        # ===== QæŠ•å½± =====
        self.W_dq = torch.nn.Parameter(0.01 * torch.randn((d_model, self.q_proj_dim)))   # å¼(QD): ä½ç§©å‹ç¼©çŸ©é˜µ W^{DQ}
        self.W_uq = torch.nn.Parameter(0.01 * torch.randn((self.q_proj_dim, d_model)))   # å¼(QU): ä½ç§©æ¢å¤çŸ©é˜µ W^{UQ}
        self.q_layernorm = torch.nn.LayerNorm(self.q_proj_dim)                           # LayerNorm(c_t^{Q})

        # ===== KVæŠ•å½± =====
        self.W_dkv = torch.nn.Parameter(0.01 * torch.randn((d_model, self.kv_proj_dim + self.qk_rope_dim)))  
        # å¼(9): [c_t^{KV}, k_{raw,t}^{R}] = W^{DKV} h_t

        self.W_ukv = torch.nn.Parameter(
            0.01 * torch.randn((self.kv_proj_dim, d_model + (n_heads * self.qk_nope_dim)))
        )  # å¼(10)+(11): [K^{C}, V^{C}] = W^{UKV} c_t^{KV}
        self.kv_layernorm = torch.nn.LayerNorm(self.kv_proj_dim)    # Ë†c_t^{KV} = LayerNorm(c_t^{KV})

        # ===== è¾“å‡ºæŠ•å½± =====
        self.W_o = torch.nn.Parameter(0.01 * torch.randn((d_model, d_model)))            # å¼(U): u_t = W^{O} o_t

        # ===== RoPE ç¼“å­˜ =====
        freqs = 1.0 / (rope_theta ** (torch.arange(0, self.dh, 2).float() / self.dh))    # Î¸_i = Î¸^{-2i/d_h}
        emb = torch.outer(torch.arange(max_len).float(), freqs)                          # Î¸_{pos,i} = pos * freq_i
        cos_cached = emb.cos()[None, None, :, :]                                         # cosÎ¸
        sin_cached = emb.sin()[None, None, :, :]                                         # sinÎ¸
        self.register_buffer("cos_cached", cos_cached)
        self.register_buffer("sin_cached", sin_cached)

    # =========================================================
    # ğŸ”¹ å‰å‘ä¼ æ’­
    # =========================================================
    def forward(self, x, kv_cache=None, past_length=0):
        B, S, D = x.size()                                                              # è¾“å…¥ h_t âˆˆ â„^{BÃ—SÃ—d_model}

        # -----------------------------------------------------
        # Step1ï¸âƒ£ KV ä½ç§©å‹ç¼©
        # å¼(9): [c_t^{KV}, k_{raw,t}^{R}] = W^{DKV} h_t
        # -----------------------------------------------------
        if kv_cache is None:
            compressed_kv = x @ self.W_dkv                                              # å¼(9): è®¡ç®— W^{DKV} h_t
            KV_for_lora, K_for_rope = torch.split(compressed_kv, [self.kv_proj_dim, self.qk_rope_dim], dim=-1)
            KV_for_lora = self.kv_layernorm(KV_for_lora)                                # Ë†c_t^{KV} = LN(c_t^{KV})
        else:
            new_kv = x @ self.W_dkv                                                     # å¼(9): å½“å‰ token çš„ [c_t^{KV}, k_{raw,t}^{R}]
            compressed_kv = torch.cat([kv_cache, new_kv], dim=1)                        # æ‹¼æ¥ç¼“å­˜ â†’ [c_{1:t}^{KV}, k_{raw,1:t}^{R}]
            new_kv, new_Kr = torch.split(new_kv, [self.kv_proj_dim, self.qk_rope_dim], dim=-1)
            old_kv, old_Kr = torch.split(kv_cache, [self.kv_proj_dim, self.qk_rope_dim], dim=-1)
            new_kv = self.kv_layernorm(new_kv)                                          # Ë†c_t^{KV}
            old_kv = self.kv_layernorm(old_kv)                                          # Ë†c_{1:t-1}^{KV}
            KV_for_lora = torch.cat([old_kv, new_kv], dim=1)                            # æ‹¼æ¥ä½ç§©æ½œå‘é‡åºåˆ—
            K_for_rope = torch.cat([old_Kr, new_Kr], dim=1)                             # æ‹¼æ¥RoPEåŸå§‹é”®åˆ†æ”¯

        # -----------------------------------------------------
        # Step2ï¸âƒ£ ä»ä½ç§©æ½œå‘é‡æ¢å¤ Key/Value
        # å¼(10) + (11): [K^{C}, V^{C}] = W^{UKV} c^{KV}
        # -----------------------------------------------------
        KV = KV_for_lora @ self.W_ukv                                                   # åº”ç”¨æ¢å¤çŸ©é˜µ W^{UKV}
        KV = KV.view(B, -1, self.n_heads, self.dh + self.qk_nope_dim).transpose(1, 2)   # reshapeæˆ[B,nH,S,dh+dh_nope]
        K, V = torch.split(KV, [self.qk_nope_dim, self.dh], dim=-1)                     # åˆ†å¾— K^{C}, V^{C}
        S_full = K.size(2)                                                              # å…¨åºåˆ—é•¿åº¦ï¼ˆå«å†å²ï¼‰

        # -----------------------------------------------------
        # Step3ï¸âƒ£ è®¡ç®— RoPE é”®åˆ†æ”¯
        # å¼(R): k_j^{R} = RoPE(k_{raw,j}^{R})
        # -----------------------------------------------------
        K_for_rope = K_for_rope.view(B, -1, 1, self.qk_rope_dim).transpose(1, 2)        # [B,1,S_full,D_rope]
        cos_k = self.cos_cached[:, :, :S_full, :self.qk_rope_dim // 2].repeat(1, 1, 1, 2)
        sin_k = self.sin_cached[:, :, :S_full, :self.qk_rope_dim // 2].repeat(1, 1, 1, 2)
        K_for_rope = apply_rope_x(K_for_rope, cos_k, sin_k)                             # k_j^{R} = RoPE(k_{raw,j}^{R})
        K_for_rope = K_for_rope.repeat(1, self.n_heads, 1, 1)                           # å¤åˆ¶åˆ°æ‰€æœ‰ head

        # -----------------------------------------------------
        # Step4ï¸âƒ£ è®¡ç®— Query
        # å¼(QD): c_t^{Q} = W^{DQ} h_t
        # å¼(QU): q_t^{C} = W^{UQ} c_t^{Q}
        # å¼(R):  q_t^{R} = RoPE(q_{raw,t}^{R})
        # -----------------------------------------------------
        compressed_q = x @ self.W_dq                                                    # å¼(QD): è®¡ç®—ä½ç§©å‹ç¼©å‘é‡ c_t^{Q}
        compressed_q = self.q_layernorm(compressed_q)                                   # Ë†c_t^{Q}
        Q = compressed_q @ self.W_uq                                                    # å¼(QU): è®¡ç®—æ¢å¤å‘é‡ q_t^{C}
        Q = Q.view(B, -1, self.n_heads, self.dh).transpose(1, 2)                        # [B,nH,S,dh]
        Q, Q_for_rope = torch.split(Q, [self.qk_nope_dim, self.qk_rope_dim], dim=-1)    # æ‹†åˆ† q_t^{C}, q_t^{R}
        cos_q = self.cos_cached[:, :, past_length:past_length + S, :self.qk_rope_dim // 2].repeat(1, 1, 1, 2)
        sin_q = self.sin_cached[:, :, past_length:past_length + S, :self.qk_rope_dim // 2].repeat(1, 1, 1, 2)
        Q_for_rope = apply_rope_x(Q_for_rope, cos_q, sin_q)                             # q_t^{R} = RoPE(q_{raw,t}^{R})

        # -----------------------------------------------------
        # Step5ï¸âƒ£ æ‹¼æ¥è§£è€¦åˆ†æ”¯
        # å¼: q_t = [q_t^{C}; q_t^{R}],  k_j = [k_j^{C}; k_j^{R}],  v_j = v_j^{C}
        # -----------------------------------------------------
        q_heads = torch.cat([Q, Q_for_rope], dim=-1)                                    # æ‹¼æ¥å¾—åˆ°å®Œæ•´ q_t
        k_heads = torch.cat([K, K_for_rope], dim=-1)                                    # æ‹¼æ¥å¾—åˆ°å®Œæ•´ k_j
        v_heads = V                                                                     # v_j ä¸å˜

        # -----------------------------------------------------
        # Step6ï¸âƒ£ æ³¨æ„åŠ›è®¡ç®—
        # å¼(A): Î±_{t,j} = softmax_j( (q_t k_j^T)/âˆšd )
        # å¼(O): o_t = Î£_j Î±_{t,j} v_j
        # -----------------------------------------------------
        mask = torch.ones((S, S_full), device=x.device)                                 # æ„é€ å› æœmask
        mask = torch.tril(mask, diagonal=past_length)
        sq_mask = mask[None, None, :, :] == 1
        x_out = F.scaled_dot_product_attention(q_heads, k_heads, v_heads, attn_mask=sq_mask)  # o_t = Î£ Î±_{t,j} v_j
        x_out = x_out.transpose(1, 2).reshape(B, S, D)                                  # åˆå¹¶æ‰€æœ‰headè¾“å‡º
        x_out = x_out @ self.W_o.T                                                      # å¼(U): u_t = W^{O} o_t

        return x_out, compressed_kv


# =========================================================
# ğŸ”¹ è°ƒè¯•å…¥å£
# =========================================================
def main():
    torch.manual_seed(42)
    d_model, n_heads, seq_len, batch = 256, 8, 8, 2
    model = MLA(d_model=d_model, n_heads=n_heads, max_len=128)
    x = torch.randn(batch, seq_len, d_model)
    out, kv = model(x)
    print(f"âœ… è¾“å…¥: {x.shape} â†’ è¾“å‡º: {out.shape}, ç¼“å­˜: {kv.shape}")

if __name__ == "__main__":
    main()
