#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSeek-V2 MLA Demo â€” å¸¦å…¬å¼æ³¨é‡Šç‰ˆæœ¬
éªŒè¯ MLA çš„ä½ç§©å‹ç¼© + è§£è€¦ RoPE
"""

import torch
import torch.nn.functional as F


# =======================================
# ğŸ”¹ RoPE æ—‹è½¬å‡½æ•°
# å…¬å¼:  RoPE(x) = xÂ·cosÎ¸ + rotate(x)Â·sinÎ¸
# =======================================
def apply_rope_x(x, cos, sin):
    """
    x: [B, H, L, D]
    cos/sin: [1, 1, L, D]
    """
    x1, x2 = x[..., ::2], x[..., 1::2]
    x_rot = torch.stack([-x2, x1], dim=-1).reshape_as(x)
    return x * cos + x_rot * sin


# =======================================
# ğŸ”¹ MLA æ¨¡å—ï¼ˆDeepSeek-V2 å®ç°ï¼‰
# =======================================
class MLA(torch.nn.Module):
    def __init__(self, d_model, n_heads, max_len=1024, rope_theta=10000.0):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.dh = d_model // n_heads
        self.q_proj_dim = d_model // 2           # â†“ å…¬å¼ (W^{DQ})
        self.kv_proj_dim = (2 * d_model) // 3    # â†“ å…¬å¼ (W^{DKV})

        # æ¯ä¸ª head çš„æ‹†åˆ†ç»´åº¦
        self.qk_nope_dim = self.dh // 2   # ä¸å¸¦ RoPE çš„éƒ¨åˆ†
        self.qk_rope_dim = self.dh // 2   # å¸¦ RoPE çš„éƒ¨åˆ†

        # ===== Q æŠ•å½±å±‚ =====
        # å…¬å¼:  c_t^Q = W^{DQ} h_t
        self.W_dq = torch.nn.Parameter(0.01 * torch.randn((d_model, self.q_proj_dim)))
        # å…¬å¼:  q_t^C = W^{UQ} c_t^Q
        self.W_uq = torch.nn.Parameter(0.01 * torch.randn((self.q_proj_dim, d_model)))
        self.q_layernorm = torch.nn.LayerNorm(self.q_proj_dim)

        # ===== KV æŠ•å½±å±‚ =====
        # å…¬å¼:  c_t^{KV} = W^{DKV} h_t
        self.W_dkv = torch.nn.Parameter(0.01 * torch.randn((d_model, self.kv_proj_dim + self.qk_rope_dim)))
        # å…¬å¼:  [K^C, V^C] = W^{UKV} c_t^{KV}
        self.W_ukv = torch.nn.Parameter(
            0.01 * torch.randn((self.kv_proj_dim, d_model + (n_heads * self.qk_nope_dim)))
        )
        self.kv_layernorm = torch.nn.LayerNorm(self.kv_proj_dim)

        # è¾“å‡ºæŠ•å½±:  u_t = W^O [o_{t,1};â€¦;o_{t,n_h}]
        self.W_o = torch.nn.Parameter(0.01 * torch.randn((d_model, d_model)))

        # ===== RoPE ç¼“å­˜ =====
        freqs = 1.0 / (rope_theta ** (torch.arange(0, self.dh, 2).float() / self.dh))
        emb = torch.outer(torch.arange(max_len).float(), freqs)
        cos_cached = emb.cos()[None, None, :, :]
        sin_cached = emb.sin()[None, None, :, :]
        self.register_buffer("cos_cached", cos_cached)
        self.register_buffer("sin_cached", sin_cached)

    # =======================================
    # å‰å‘ä¼ æ’­
    # =======================================
    def forward(self, x, kv_cache=None, past_length=0):
        """
        è¾“å…¥:  x âˆˆ â„^{BÃ—SÃ—d_model}
        è¾“å‡º:  u_t, c^{KV}
        """
        B, S, D = x.size()

        # -------------------------------------------------
        # Step1ï¸âƒ£ KV ä½ç§©å‹ç¼©
        # -------------------------------------------------
        # å…¬å¼:  c_j^{KV} = W^{DKV} h_j
        if kv_cache is None:
            compressed_kv = x @ self.W_dkv
            # æ‹†åˆ†:  KV_for_lora â†’ c^{KV};  K_for_rope â†’ RoPEéƒ¨åˆ†
            KV_for_lora, K_for_rope = torch.split(compressed_kv,
                                                  [self.kv_proj_dim, self.qk_rope_dim], dim=-1)
            KV_for_lora = self.kv_layernorm(KV_for_lora)
        else:
            # æ¨ç†é˜¶æ®µ: æ‹¼æ¥æ—§ç¼“å­˜
            new_kv = x @ self.W_dkv
            compressed_kv = torch.cat([kv_cache, new_kv], dim=1)
            new_kv, new_K_for_rope = torch.split(new_kv, [self.kv_proj_dim, self.qk_rope_dim], dim=-1)
            old_kv, old_K_for_rope = torch.split(kv_cache, [self.kv_proj_dim, self.qk_rope_dim], dim=-1)
            new_kv = self.kv_layernorm(new_kv)
            old_kv = self.kv_layernorm(old_kv)
            KV_for_lora = torch.cat([old_kv, new_kv], dim=1)
            K_for_rope = torch.cat([old_K_for_rope, new_K_for_rope], dim=1)

        # -------------------------------------------------
        # Step2ï¸âƒ£ ä½ç§©æ¢å¤ Key/Value
        # -------------------------------------------------
        # å…¬å¼:  [K^C, V^C] = W^{UKV} c^{KV}
        KV = KV_for_lora @ self.W_ukv
        KV = KV.view(B, -1, self.n_heads, self.dh + self.qk_nope_dim).transpose(1, 2)
        # æ‹†åˆ†:  K^C, V^C
        K, V = torch.split(KV, [self.qk_nope_dim, self.dh], dim=-1)
        S_full = K.size(2)

        # -------------------------------------------------
        # Step3ï¸âƒ£ è®¡ç®— RoPE Key
        # -------------------------------------------------
        # å…¬å¼:  k_j^R = RoPE(W^{KR} h_j)
        K_for_rope = K_for_rope.view(B, -1, 1, self.qk_rope_dim).transpose(1, 2)
        cos_k = self.cos_cached[:, :, :S_full, :self.qk_rope_dim // 2].repeat(1, 1, 1, 2)
        sin_k = self.sin_cached[:, :, :S_full, :self.qk_rope_dim // 2].repeat(1, 1, 1, 2)
        K_for_rope = apply_rope_x(K_for_rope, cos_k, sin_k)
        K_for_rope = K_for_rope.repeat(1, self.n_heads, 1, 1)

        # -------------------------------------------------
        # Step4ï¸âƒ£ è®¡ç®— Queryï¼ˆå«è§£è€¦ RoPEï¼‰
        # -------------------------------------------------
        # (a) å‹ç¼©:  c_t^Q = W^{DQ} h_t
        compressed_q = x @ self.W_dq
        compressed_q = self.q_layernorm(compressed_q)
        # (b) æ¢å¤:  q_t^C = W^{UQ} c_t^Q
        Q = compressed_q @ self.W_uq
        Q = Q.view(B, -1, self.n_heads, self.dh).transpose(1, 2)
        # (c) æ‹†åˆ†:  Q=[Q_nope, Q_rope]
        Q, Q_for_rope = torch.split(Q, [self.qk_nope_dim, self.qk_rope_dim], dim=-1)
        # (d) åŠ  RoPE:  q_t^R = RoPE(W^{QR} c_t^Q)
        cos_q = self.cos_cached[:, :, past_length:past_length + S, :self.qk_rope_dim // 2].repeat(1, 1, 1, 2)
        sin_q = self.sin_cached[:, :, past_length:past_length + S, :self.qk_rope_dim // 2].repeat(1, 1, 1, 2)
        Q_for_rope = apply_rope_x(Q_for_rope, cos_q, sin_q)

        # -------------------------------------------------
        # Step5ï¸âƒ£ æ‹¼æ¥è§£è€¦åˆ†æ”¯
        # -------------------------------------------------
        # å…¬å¼:  q_t = [q_t^C ; q_t^R],  k_j = [k_j^C ; k_j^R]
        q_heads = torch.cat([Q, Q_for_rope], dim=-1)
        k_heads = torch.cat([K, K_for_rope], dim=-1)
        v_heads = V

        # -------------------------------------------------
        # Step6ï¸âƒ£ æ³¨æ„åŠ›æ‰“åˆ†ä¸åŠ æƒ
        # -------------------------------------------------
        # å…¬å¼:  Î±_{t,j} = softmax_j( (q_t k_j^T)/âˆš(d_h+d_h^R) )
        mask = torch.ones((S, S_full), device=x.device)
        mask = torch.tril(mask, diagonal=past_length)
        sq_mask = mask[None, None, :, :] == 1
        x_out = F.scaled_dot_product_attention(q_heads, k_heads, v_heads, attn_mask=sq_mask)
        # å…¬å¼:  o_t = Î£_j Î±_{t,j} v_j^C
        x_out = x_out.transpose(1, 2).reshape(B, S, D)
        # è¾“å‡º:  u_t = W^O [o_{t,1};â€¦;o_{t,n_h}]
        x_out = x_out @ self.W_o.T

        return x_out, compressed_kv


# =======================================
# ğŸ”¹ è°ƒè¯•å…¥å£
# =======================================
def main():
    torch.manual_seed(42)
    d_model, n_heads, seq_len, batch = 256, 8, 8, 2
    model = MLA(d_model=d_model, n_heads=n_heads, max_len=128)
    x = torch.randn(batch, seq_len, d_model)
    out, kv = model(x)
    print(f"âœ… è¾“å…¥: {x.shape} â†’ è¾“å‡º: {out.shape}, ç¼“å­˜: {kv.shape}")

if __name__ == "__main__":
    main()
