import torch
import torch.nn as nn
import torch.nn.functional as F

##########################################
# ğŸ§® GAEï¼ˆå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼‰å…¬å¼å¯¹åº”ä»£ç 
##########################################
def compute_advantage(gamma, lmbda, td_delta):
    """
    è®¡ç®— GAEï¼š A_t = Î£ (Î³Î»)^l * Î´_{t+l}
    å‚æ•°ï¼š
        gammaï¼šæŠ˜æ‰£å› å­
        lmbdaï¼šGAE è¡°å‡ç³»æ•°
        td_deltaï¼šTD æ®‹å·® Î´_t = r + Î³ V(s') - V(s)
    è¿”å›ï¼š
        advantageï¼šæ¯ä¸ªæ—¶é—´æ­¥çš„ä¼˜åŠ¿å€¼
    """
    td_delta = td_delta.detach().cpu().numpy()
    advantage = 0.0
    advantage_list = []
    for delta in reversed(td_delta):
        advantage = gamma * lmbda * advantage + delta
        advantage_list.append(advantage)
    advantage_list.reverse()
    return torch.tensor(advantage_list, dtype=torch.float32)

##########################################
# ğŸ§© ç­–ç•¥ç½‘ç»œçš„ PPO Lossï¼ˆå…¬å¼æ ¸å¿ƒå®ç°ï¼‰
##########################################
class PolicyLoss(nn.Module):
    """
    ç­–ç•¥æŸå¤±å‡½æ•°ï¼Œå¯¹åº” PPO çš„ min/clip ç›®æ ‡å‡½æ•°
    """
    def __init__(self, clip_eps: float = 0.2):
        super().__init__()
        self.clip_eps = clip_eps

    def forward(self, log_probs, old_log_probs, advantages, action_mask=None):
        ratio = (log_probs - old_log_probs).exp()  # e^{logÏ€ - logÏ€_old}
        surr1 = ratio * advantages
        surr2 = ratio.clamp(1 - self.clip_eps, 1 + self.clip_eps) * advantages
        loss = -torch.min(surr1, surr2)
        loss = masked_mean(loss, action_mask, dim=-1).mean()
        return loss

##########################################
# ğŸ“ˆ å€¼å‡½æ•°æŸå¤±ï¼ˆValue Function Clippingï¼‰
##########################################
class ValueLoss(nn.Module):
    """
    å€¼å‡½æ•°æŸå¤±ï¼Œå¯¹ value ä½¿ç”¨ clip é¿å…æ›´æ–°è¿‡å¤§
    """
    def __init__(self, clip_eps: float = 0.2):
        super().__init__()
        self.clip_eps = clip_eps

    def forward(self, values, old_values, returns, action_mask=None):
        values_clipped = old_values + (values - old_values).clamp(-self.clip_eps, self.clip_eps)
        surr1 = (values_clipped - returns) ** 2
        surr2 = (values - returns) ** 2
        loss = torch.max(surr1, surr2)
        loss = masked_mean(loss, action_mask, dim=-1).mean()
        return 0.5 * loss

##########################################
# ğŸ¤¼ Pairwise Lossï¼šç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹ RM
##########################################
class PairWiseLoss(nn.Module):
    """
    ç”¨äº reward model çš„ pairwise è®­ç»ƒ
    chosen_reward åº”è¯¥é«˜äº reject_reward
    """
    def forward(self, chosen_reward, reject_reward, margin=None):
        if margin is not None:
            loss = -F.logsigmoid(chosen_reward - reject_reward - margin)
        else:
            loss = -F.logsigmoid(chosen_reward - reject_reward)
        return loss.mean()

##########################################
# ğŸ’° å¥–åŠ±è®¡ç®—ï¼šr + KL ç½šé¡¹ï¼ˆæˆ–åŠ  bonusï¼‰
##########################################
def compute_reward(r, kl, action_mask, kl_coef=0.01):
    """
    å¥–åŠ± = r - Î² * KLï¼Œr åªåŠ åˆ°æœ€åä¸€ä¸ª stepï¼ˆä¾‹å¦‚ EOSï¼‰
    """
    kl_reward = -kl_coef * kl
    eos_idx = action_mask.size(1) - 1 - action_mask.long().fliplr().argmax(dim=1, keepdim=True)
    last_reward = torch.zeros_like(kl).scatter(1, eos_idx, r.unsqueeze(1).to(kl.dtype))
    total_reward = last_reward + kl_reward
    return total_reward

##########################################
# ğŸ”¢ mask å¹³å‡å‡½æ•°ï¼Œç”¨äºå¿½ç•¥ padding token
##########################################
def masked_mean(tensor, mask=None, dim=-1):
    if mask is None:
        return tensor.mean(dim)
    return (tensor * mask).sum(dim) / mask.sum(dim)

##########################################
# ğŸ” PPO æ›´æ–°ç¤ºä¾‹
##########################################
def ppo_train_step(
    policy_model,         # ç­–ç•¥ç½‘ç»œ
    value_model,          # å€¼ç½‘ç»œ
    optimizer,            # ä¼˜åŒ–å™¨
    states,               # çŠ¶æ€ s_tï¼ˆæˆ– promptï¼‰
    actions,              # è¡ŒåŠ¨ a_tï¼ˆæˆ–ç”Ÿæˆ tokenï¼‰
    old_log_probs,        # è€ç­–ç•¥ logÏ€_old
    returns,              # GAE åŠ æƒåçš„ Return
    advantages,           # GAE Advantage
    values,               # è€çš„å€¼ä¼°è®¡
    action_mask=None,     # mask æ‰ padding
    kl=None,              # KL divergence
    ref_log_probs=None    # å‚è€ƒç­–ç•¥ï¼ˆå¯é€‰ï¼‰
):
    # forward current policy
    log_probs = policy_model.get_log_prob(states, actions)
    new_values = value_model(states)

    # policy loss
    policy_loss = PolicyLoss()(log_probs, old_log_probs, advantages, action_mask)

    # value loss
    value_loss = ValueLoss()(new_values, values, returns, action_mask)

    # total loss
    loss = policy_loss + value_loss

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return {
        'policy_loss': policy_loss.item(),
        'value_loss': value_loss.item(),
        'total_loss': loss.item()
    }
